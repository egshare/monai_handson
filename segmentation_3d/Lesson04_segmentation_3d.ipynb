{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4",
   "authorship_tag": "ABX9TyOKJ2LwDdKxz+pxApTtQpwl",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/egshare/monai_handson/blob/master/segmentation_3d/Lesson04_segmentation_3d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copyright (c) e-Growth Co., Ltd.\n",
    "\n",
    "このノートブックはMITライセンスの下で公開されています。\n",
    "利用、変更、配布は許可されますが、著作権表示は必須です。\n",
    "詳細はMITライセンスの公式文書を参照してください。\n",
    "\n",
    "このノートブック内で使用している一部のコードは、MONAI チュートリアル に基づいています。\n",
    "Copyright (c) MONAI Consortium\n",
    "MONAIの詳細なライセンス情報は、[ https://github.com/Project-MONAI/tutorials ] で確認できます。\n",
    "\n",
    "# MONAIを利用した脾臓の3Dセグメンテーション学習(3D UNet利用)\n",
    "\n",
    "本チュートリアルにおいて、高い精度を得るには数時間以上をかけて学習を行いますので、時間に余裕がある時に試してください。\n",
    "\n",
    "本チュートリアルは [Project-MONAI](https://github.com/Project-MONAI) の脾臓のセグメンテーションチュートリアルを簡略化し、一部修正したものです。\n",
    "\n",
    "前回までと違い、このチュートリアルでは、3D UNetを利用した学習を行います。スライスベースの学習では学習時にランダムに2Dスライスをサンプリングして学習を行いましたが、3D UNetに対して学習を行う場合、ランダムに3D部分画像を習得して学習を行います。"
   ],
   "metadata": {
    "id": "q7CJ8ecMPFzE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 必要なライブラリのインストールを行います"
   ],
   "metadata": {
    "id": "IguQIP9nQIYm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSpY6aqeOo13"
   },
   "outputs": [],
   "source": [
    "!pip install -U SimpleITK \"monai[ignote, nibabel, torchvision, tdqm]==1.1.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 必要なライブラリのインポート"
   ],
   "metadata": {
    "id": "GN6V6-51QONq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print_config()"
   ],
   "metadata": {
    "id": "GsJWKZHXQXJS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データディレクトリの設定"
   ],
   "metadata": {
    "id": "ad7z4IzdQxLM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "#root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "root_dir = \"output_root/\"\n",
    "os.makedirs(root_dir, exist_ok=True)\n",
    "print(root_dir)"
   ],
   "metadata": {
    "id": "wb83DyDKQzpY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データのダウンロード\n",
    "MICCAIのMedical Segmentation Decathlon ( [http://medicaldecathlon.com/](http://medicaldecathlon.com/) ) で提供されたSpleenデータを利用します。"
   ],
   "metadata": {
    "id": "T6J2URZYQ3n8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "resource = \"https://msd-for-monai.s3-us-west-2.amazonaws.com/Task09_Spleen.tar\"\n",
    "md5 = \"410d4a301da4e5b2f6f86ec3ddba524e\"\n",
    "\n",
    "compressed_file = os.path.join(root_dir, \"Task09_Spleen.tar\")\n",
    "data_dir = os.path.join(root_dir, \"Task09_Spleen\")\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir, md5)"
   ],
   "metadata": {
    "id": "X27VWnyFSAKq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データセットの読み込み用パスを構成する\n",
    "imagesTrフォルダ以下のniiファイルが画像で、labelsTr以下のniiファイルが脾臓のラベルになります。\n",
    "\n",
    "※MICCAIでは現在もこの構成でチャレンジデータを配布しています。"
   ],
   "metadata": {
    "id": "gT99675VSZi0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "\n",
    "# 後ろから9個のデータを検証データとし、それ以外を学習データとします\n",
    "train_files, val_files = data_dicts[:-9], data_dicts[-9:]"
   ],
   "metadata": {
    "id": "p-vRq2w5SsFd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformの準備"
   ],
   "metadata": {
    "id": "Yvvw6xsZS4at"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# サンプリングされる部分画像の１辺あたりのピクセル数\n",
    "# サンプルデータの解像度を超えないサイズ(4の倍数推奨)に設定してください\n",
    "# GPUメモリが足りない場合はこのサイズを小さくしましょう\n",
    "roi_size = 96\n",
    "\n",
    "# 着目するCT値の範囲\n",
    "# 学習対象臓器に応じて設定することでサンプル不足による学習精度の低下をある程度抑制できる\n",
    "img_val_min, img_val_max = -57, 164\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=img_val_min,\n",
    "            a_max=img_val_max,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(roi_size, roi_size, roi_size),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=4,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        # user can also add other random transforms\n",
    "        # RandAffined(\n",
    "        #     keys=['image', 'label'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     prob=1.0, spatial_size=(roi_size, roi_size, roi_size),\n",
    "        #     rotate_range=(0, 0, np.pi/15),\n",
    "        #     scale_range=(0.1, 0.1, 0.1)),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=img_val_min,\n",
    "            a_max=img_val_max,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1.5, 1.5, 2.0), mode=(\"bilinear\", \"nearest\")),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "YBXSYMLNS-wZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformの出力を確認"
   ],
   "metadata": {
    "id": "HKHXX6AdULpa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "check_data = first(check_loader)\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")\n",
    "# plot the slice [:, :, 80]\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(image[:, :, 80], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 80])\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "r5Iv1cmfUPhu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## データローダーを定義"
   ],
   "metadata": {
    "id": "OoHMvpiaVpcM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## DataLoader(学習や検証時にモデルにサンプルを供給するクラス)の設定\n",
    "# キャッシュメモリに応じて0以上～1.0までを設定\n",
    "# データ数が多い場合、1.0にするとかなりシステムメモリを圧迫するので、適宜調整\n",
    "cache_rate = 1.0\n",
    "\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=cache_rate, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=cache_rate, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
   ],
   "metadata": {
    "id": "3TFxwZfhVnDK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## モデル、損失関数、オプティマイザーの定義"
   ],
   "metadata": {
    "id": "o3tkqsJjWDHN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "## 3D UNetとして定義\n",
    "model = UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    "    norm=Norm.BATCH,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "dice_metric = DiceMetric(include_background=False, reduction=\"mean\")"
   ],
   "metadata": {
    "id": "huZroDnOWItW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 学習の実施\n",
    "\n",
    "学習ループは前回までのチュートリアルとほぼ同じです\n",
    "ただし、学習時間は2Dモデルと比べると長くなります。"
   ],
   "metadata": {
    "id": "0khYiPbLWSPu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "max_epochs = 50\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "post_pred = Compose([AsDiscrete(argmax=True, to_onehot=2)])\n",
    "post_label = Compose([AsDiscrete(to_onehot=2)])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                roi_size = (160, 160, 160)\n",
    "                sw_batch_size = 4\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model)\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            dice_metric.reset()\n",
    "\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )"
   ],
   "metadata": {
    "id": "fbnAgWOGWUPR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")"
   ],
   "metadata": {
    "id": "fzElOUgPXZvL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lossおよび精度の曲線をプロットしてみる"
   ],
   "metadata": {
    "id": "uYVyGDVMXfaR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "YVqUtLZBXmFb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 最高スコアが得られたモデルで推論してみる"
   ],
   "metadata": {
    "id": "mLxXzy5PX-r1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, val_data in enumerate(val_loader):\n",
    "        roi_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_outputs = sliding_window_inference(val_data[\"image\"].to(device), roi_size, sw_batch_size, model)\n",
    "        # plot the slice [:, :, 80]\n",
    "        plt.figure(\"check\", (18, 6))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f\"image {i}\")\n",
    "        plt.imshow(val_data[\"image\"][0, 0, :, :, 80], cmap=\"gray\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title(f\"label {i}\")\n",
    "        plt.imshow(val_data[\"label\"][0, 0, :, :, 80])\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title(f\"output {i}\")\n",
    "        plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, 80])\n",
    "        plt.show()\n",
    "        if i == 2:\n",
    "            break"
   ],
   "metadata": {
    "id": "qYJPJUAPYEUk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 元データのSpacingで推論精度を評価する"
   ],
   "metadata": {
    "id": "3jUZ0gO_YNrT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "val_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=img_val_min,\n",
    "            a_max=img_val_max,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_org_ds = Dataset(data=val_files, transform=val_org_transforms)\n",
    "val_org_loader = DataLoader(val_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=val_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "            device=\"cpu\",\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "        AsDiscreted(keys=\"label\", to_onehot=2),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "eO1Bau7XYUI2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for val_data in val_org_loader:\n",
    "        val_inputs = val_data[\"image\"].to(device)\n",
    "        win_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        val_data[\"pred\"] = sliding_window_inference(val_inputs, win_size, sw_batch_size, model)\n",
    "        val_data = [post_transforms(i) for i in decollate_batch(val_data)]\n",
    "        val_outputs, val_labels = from_engine([\"pred\", \"label\"])(val_data)\n",
    "        # compute metric for current iteration\n",
    "        dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "    # aggregate the final mean dice result\n",
    "    metric_org = dice_metric.aggregate().item()\n",
    "    # reset the status for next validation round\n",
    "    dice_metric.reset()\n",
    "\n",
    "print(\"Metric on original image spacing: \", metric_org)"
   ],
   "metadata": {
    "id": "pZfJGJvRYVFE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## テストセットに対して推論してみる\n",
    "\n",
    "Spleenデータには正解ラベルのないデータとしてimagesTsフォルダ以下で提供されている。これらのデータに対して学習済みモデルで推論してみましょう"
   ],
   "metadata": {
    "id": "0Fhj0g5SYX-U"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTs\", \"*.nii.gz\")))\n",
    "\n",
    "test_data = [{\"image\": image} for image in test_images]\n",
    "\n",
    "\n",
    "test_org_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=\"image\"),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\"], pixdim=(1.5, 1.5, 2.0), mode=\"bilinear\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=img_val_min,\n",
    "            a_max=img_val_max,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_org_ds = Dataset(data=test_data, transform=test_org_transforms)\n",
    "\n",
    "test_org_loader = DataLoader(test_org_ds, batch_size=1, num_workers=4)\n",
    "\n",
    "post_transforms = Compose(\n",
    "    [\n",
    "        Invertd(\n",
    "            keys=\"pred\",\n",
    "            transform=test_org_transforms,\n",
    "            orig_keys=\"image\",\n",
    "            meta_keys=\"pred_meta_dict\",\n",
    "            orig_meta_keys=\"image_meta_dict\",\n",
    "            meta_key_postfix=\"meta_dict\",\n",
    "            nearest_interp=False,\n",
    "            to_tensor=True,\n",
    "        ),\n",
    "        AsDiscreted(keys=\"pred\", argmax=True, to_onehot=2),\n",
    "        SaveImaged(keys=\"pred\", meta_keys=\"pred_meta_dict\", output_dir=\"./out\", output_postfix=\"seg\", resample=False),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "id": "JI2ex-sAYmuu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from monai.transforms import LoadImage\n",
    "loader = LoadImage()"
   ],
   "metadata": {
    "id": "ECat4IvqYqa4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_data in test_org_loader:\n",
    "        test_inputs = test_data[\"image\"].to(device)\n",
    "        win_size = (160, 160, 160)\n",
    "        sw_batch_size = 4\n",
    "        test_data[\"pred\"] = sliding_window_inference(test_inputs, win_size, sw_batch_size, model)\n",
    "\n",
    "        test_data = [post_transforms(i) for i in decollate_batch(test_data)]\n",
    "\n",
    "        ## 推論結果の表示\n",
    "        if True:\n",
    "          test_output = from_engine([\"pred\"])(test_data)\n",
    "          original_image = loader(test_output[0].meta[\"filename_or_obj\"])\n",
    "\n",
    "          plt.figure(\"check\", (18, 6))\n",
    "          plt.subplot(1, 2, 1)\n",
    "          plt.imshow(original_image[0][:, :, 20], cmap=\"gray\")\n",
    "          plt.subplot(1, 2, 2)\n",
    "          plt.imshow(test_output[0].detach().cpu()[1, :, :, 20])\n",
    "          plt.show()"
   ],
   "metadata": {
    "id": "JKm0CK0UYtv4"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}